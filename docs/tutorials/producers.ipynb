{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62e20be3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c7206b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from openseize import producer\n",
    "from openseize.core.producer import as_producer\n",
    "from openseize import demos\n",
    "from openseize.file_io import edf, annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17356ab3",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d0395d",
   "metadata": {},
   "source": [
    "<font size=3>The size of an EEG  dataset depends on three factors, the number of signals acquired, the sampling rate of each signal and the duration of the measurement. Recent advances in electrode and data acquistion hardware allow for increases in each of these factors such that the resulting dataset may not fit into the virtual (RAM) memory of a user's computer.</font>\n",
    "\n",
    "<font size=3>To address this, openseize uses an iterable object -- the <font color='darkcyan'> <b>producer, an object that sequentially produces numpy arrays from a data source</b></font>. This data source can be a sequence, an ndarray, a file stored to disk, or even a generator function that produces data itself. In this demo, we will cover producer creation routines, attributes, and methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cc2077",
   "metadata": {},
   "source": [
    "## Creation Routines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3069cb",
   "metadata": {},
   "source": [
    "<font size=3> All producers, no matter the data source, are constructed using the produce() function. To see what arguments are needed to build a producer, we can look at the producer function documentation.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f2cd834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function producer in module openseize.core.producer:\n",
      "\n",
      "producer(data: Union[numpy.ndarray[Any, numpy.dtype[+ScalarType]], Iterable[numpy.ndarray[Any, numpy.dtype[+ScalarType]]], openseize.file_io.edf.Reader, Callable, ForwardRef('Producer')], chunksize: int, axis: int, shape: Optional[Sequence[int]] = None, mask: Optional[numpy.ndarray[Any, numpy.dtype[numpy.bool_]]] = None, **kwargs) -> 'Producer'\n",
      "    Constructs an iterable that produces ndarrays of length chunksize\n",
      "    along axis during iteration.\n",
      "    \n",
      "    This constructor returns an object that is capable of producing ndarrays\n",
      "    or masked ndarrays during iteration from a single ndarray, a sequence of\n",
      "    ndarrays, a file Reader instance (see io.bases.Reader), an ndarray\n",
      "    generating function, or a pre-existing producer of ndarrays. The\n",
      "    produced ndarrays from this object will have length chunksize along axis.\n",
      "    \n",
      "    Args:\n",
      "        data:\n",
      "            An object from which ndarrays will be produced from. Supported\n",
      "            types are Reader instances, ndarrays, sequences, generating\n",
      "            functions yielding ndarrays, or other Producers.  For sequences\n",
      "            and generator functions it is required that each subarray has\n",
      "            the same shape along all axes except for the axis along which\n",
      "            chunks will be produced.\n",
      "        chunksize:\n",
      "            The desired length along axis of each produced ndarray.\n",
      "        axis:\n",
      "            The sample axis of data that will be partitioned into\n",
      "            chunks of length chunksize.\n",
      "        shape:\n",
      "            The combined shape of all ndarrays from this producer. This\n",
      "            parameter is only required when object is a generating function\n",
      "            and will be ignored otherwise.\n",
      "        mask:\n",
      "            A boolean describing which values of data along axis\n",
      "            should by produced. Values that are True will be produced and\n",
      "            values that are False will be ignored. If None (Default),\n",
      "            producer will produce all values from object.\n",
      "        kwargs:\n",
      "            Keyword arguments specific to data type that ndarrays will be\n",
      "            produced from. For Reader instances, valid kwargs are padvalue\n",
      "            (see io.bases.Readers and io.edf.Reader) For generating\n",
      "            functions, all the positional and keyword arguments must be\n",
      "            passed to the function through these kwargs to avoid name\n",
      "            collisions with the producer func arguments.\n",
      "    \n",
      "    Returns: An iterable of ndarrays of shape chunksize along axis.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(producer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620ff6e2",
   "metadata": {},
   "source": [
    "<font size=3>So the producer function needs a <font color='darkcyan'>data</font> source, a <font color='darkcyan'>chunksize</font> describing the number of samples that should be included in each produced subarray, the <font color='darkcyan'>axis</font> along which samples lie, and <font color='darkcyan'>possibly a shape and mask</font>. We will cover each of these parameters in detail in this demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb189cc6",
   "metadata": {},
   "source": [
    "### Producers From Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3763f8",
   "metadata": {},
   "source": [
    "<font size=3>To create a producer from an array may seem silly. <i>Isn't the array already in memory?</i> Well, yes it is but maybe that array is consuming a lot of your memory and you can't do anything with the array. By creating a producer, you can work with the produced values using any of the openseize functions (downsample, filter, etc) while still holding the array in-memory. <br/><br/> <b>Let's make an array and then create a producer to demonstrate this utility.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c5eac3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data is using = 32.0 MB\n"
     ]
    }
   ],
   "source": [
    "# create a reproducible random data array with 4 channels and 1 million samples along axis=1\n",
    "rng = np.random.default_rng(1234)\n",
    "data = rng.random((4, 1000000))\n",
    "\n",
    "# lets also print data's memory consumption\n",
    "print('data is using = {} MB'.format((data.size * data.itemsize)/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc43f86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pro is using = 48 Bytes\n"
     ]
    }
   ],
   "source": [
    "# build a producer declaring that we want the producer to yield arrays of size 300000 \n",
    "# using the samples along the last axis\n",
    "pro = producer(data, chunksize=300000, axis=-1)\n",
    "\n",
    "# lets checkout the producer's memory consumption\n",
    "print('pro is using = {} Bytes'.format(sys.getsizeof(pro)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53521d3e",
   "metadata": {},
   "source": [
    "<font size=3>This is the first important point about producers. <font color='darkcyan'><b>Producers do not store data, they are iterables that know how to yield data to you on-the-fly.</b></font> Let's see what the producers attributes are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "249e3849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArrayProducer Object\n",
      "---Attributes & Properties---\n",
      "{'data': array([[0.97669977, 0.38019574, 0.92324623, ..., 0.02049864, 0.84033509,\n",
      "        0.07061386],\n",
      "       [0.32584251, 0.01559622, 0.16734471, ..., 0.48613722, 0.13466647,\n",
      "        0.78129557],\n",
      "       [0.45169665, 0.44011763, 0.0325013 , ..., 0.86914401, 0.5904367 ,\n",
      "        0.4616979 ],\n",
      "       [0.84830865, 0.97995714, 0.63405179, ..., 0.7236714 , 0.80536627,\n",
      "        0.77495984]]),\n",
      " 'axis': -1,\n",
      " 'kwargs': {},\n",
      " 'chunksize': 300000,\n",
      " 'shape': (4, 1000000)}\n",
      "\n",
      "Type help(ArrayProducer) for full documentation\n"
     ]
    }
   ],
   "source": [
    "print(pro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817fcf3b",
   "metadata": {},
   "source": [
    "<font size=3>The producer instance is holding a reference to the data array, the sample axis, the chunksize of subarrays that will be produced, and the shape of the referenced data. Let's try to get each subarray from the producer. <font color='firebrick'><i>Wait.. how do we do that?</i></font>. \n",
    "<br>\n",
    "<br>\n",
    "Since the producer is an iterable, you can access each subarray just like any iterable, Any method that triggers python's iteration protocol will give you the subarrays in the producer. This could be a for-loop, a list comprehension, or an explicit call to the iter and next builtin methods. <b>Lets access each produced array in a loop.</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c0b2b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array 0, shape=(4, 300000)\n",
      "[[0.97669977 0.38019574 0.92324623 0.26169242 0.31909706]\n",
      " [0.32584251 0.01559622 0.16734471 0.12427613 0.25749222]\n",
      " [0.45169665 0.44011763 0.0325013  0.02906749 0.20707769]\n",
      " [0.84830865 0.97995714 0.63405179 0.71921724 0.34165105]]\n",
      "Array 1, shape=(4, 300000)\n",
      "[[0.19975295 0.38469445 0.31663237 0.32026263 0.85713905]\n",
      " [0.68094421 0.67678136 0.02969927 0.90235448 0.79731081]\n",
      " [0.3700237  0.60763138 0.04216831 0.57699506 0.04456521]\n",
      " [0.54071085 0.82855925 0.09775676 0.03968656 0.65453465]]\n",
      "Array 2, shape=(4, 300000)\n",
      "[[0.92858655 0.05528663 0.88124263 0.28606888 0.54164412]\n",
      " [0.95592965 0.80143229 0.09263899 0.72895997 0.85988591]\n",
      " [0.7104101  0.58855675 0.11348623 0.5171883  0.90972664]\n",
      " [0.48743344 0.00490091 0.20384552 0.91139126 0.04721849]]\n",
      "Array 3, shape=(4, 100000)\n",
      "[[0.78483056 0.93115015 0.41382943 0.38030702 0.75412888]\n",
      " [0.4725766  0.14425412 0.15515715 0.71459954 0.30351422]\n",
      " [0.34821652 0.89459182 0.1399783  0.21133067 0.58058115]\n",
      " [0.78146378 0.0234853  0.10318636 0.26773979 0.75606789]]\n"
     ]
    }
   ],
   "source": [
    "# loop to access each subarray printing it's shape and first 5 of samples for each channel\n",
    "for idx, subarr in enumerate(pro):\n",
    "    print('Array {}, shape={}'.format(idx, subarr.shape))\n",
    "    print(subarr[:, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ad75f2",
   "metadata": {},
   "source": [
    "<font size=3>Be sure not to miss that the last array the producer yielded was smaller than the previous 3. Why? Remember the data shape is (4, 1e6) and 1e6 is not perfectly divisible by 300,000. In fact, the last array yielded is of course 1e6 % 300,000 = 100,000 samples long. Important question <b>Is the producer exhausted?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff4fa849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array 0, shape=(4, 300000)\n",
      "Array 1, shape=(4, 300000)\n",
      "Array 2, shape=(4, 300000)\n",
      "Array 3, shape=(4, 100000)\n"
     ]
    }
   ],
   "source": [
    "# test if producer can produce again\n",
    "for idx, subarr in enumerate(pro):\n",
    "    print('Array {}, shape={}'.format(idx, subarr.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dde004f",
   "metadata": {},
   "source": [
    "<font size=3>This is critical: <font color='darkcyan'><b>the producer is an iterable not a one-shot iterator. It can go through the data as many times as you need.</b></font> \n",
    "<br>\n",
    "<br> \n",
    "Now if you are skeptical (like any good scientist) you are probably wondering. <i>How do I know that the produced values <b>exactly</b> match the original data source.</i> <b>Let's demonstrate that all the produced data exactly matches the original data source.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "492adc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fingers crossed.. Do they match? -> True\n"
     ]
    }
   ],
   "source": [
    "# demonstrate that the  produced arrays match the original data source 'data'\n",
    "# concatenate all produced subarrays along the last sample axis.\n",
    "produced_array = np.concatenate([subarr for subarr in pro], axis=1)\n",
    "\n",
    "#now test if the combined produced arrays match the original data array\n",
    "print('Fingers crossed.. Do they match? -> {}'.format(np.allclose(produced_array, data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df76e02d",
   "metadata": {},
   "source": [
    "<font size=3> Our method of testing array equality required us to concatenate the produced arrays. Since converting a producer to an ndarray is likely something you'll need often, it is a formal method of each producer instance called  <i>to_array</i>. <b>Let's call this important method and repeat our test.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5140de88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match? -> True\n"
     ]
    }
   ],
   "source": [
    "# demonstrate that the  produced arrays match the original data source 'data'\n",
    "# concatenate all produced subarrays along the last sample axis using the producer's to_array method.\n",
    "produced_array = pro.to_array()\n",
    "\n",
    "#now test if the combined produced arrays match the original data array\n",
    "print('Match? -> {}'.format(np.allclose(produced_array, data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aa189a",
   "metadata": {},
   "source": [
    "<font size=3>Of course this was just one test. If you need to see more tests to be convinced, please see openseize.core.tests.producer_tests for the formal pytesting.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a442cc3f",
   "metadata": {},
   "source": [
    "### Producers From Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809a03fc",
   "metadata": {},
   "source": [
    "<font size=3>As you might guess from our discussion on producers built from arrays, producers can be built from any sequence. <b>Let's show that producers can be built from sequences.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bc84191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a fun sequence from monty-python\n",
    "my_seq = [[\"(Knight) Tis but a scratch.\"],\n",
    "          [\"(Arthur) A scratch? Your arm's off!\"],\n",
    "          [\"(Knight) No, it isn't.\"],\n",
    "          [\"(Arthur) Well, what's that then?\"]]\n",
    "# convert it to a scene producer\n",
    "scene = producer(my_seq, chunksize=1, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5036af27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(Knight) Tis but a scratch.']\n",
      "[\"(Arthur) A scratch? Your arm's off!\"]\n",
      "[\"(Knight) No, it isn't.\"]\n",
      "[\"(Arthur) Well, what's that then?\"]\n"
     ]
    }
   ],
   "source": [
    "# play the scene out\n",
    "for dialog in scene:\n",
    "    print(dialog)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60234f64",
   "metadata": {},
   "source": [
    "<font size=3>There is no restriction on the datatype that can be produced as the above snippet demonstrates. As such, you might find producers useful for other large tasks that you need to break into subproblems.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fe319b",
   "metadata": {},
   "source": [
    "### Producers From Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3756276",
   "metadata": {},
   "source": [
    "<font size=3>Producing data from a file stored to disk that is too large to fit into virtual memory is one of the  most important use cases for producers. Here we are going to open a European data format(+) binary file type and produce arrays from it. A detailed demo of this important file reader can be found in the file_reading demo.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71640f25",
   "metadata": {},
   "source": [
    "#### Demo Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9f6e6e",
   "metadata": {},
   "source": [
    "<font size=3>In order to produce from a file, we will need demo data. Openseize includes a sample edf file called <i>recording_001.edf</i>. <font color='firebrick'><b>Where is this file?</b></font>\n",
    "\n",
    "<font size=3>When we imported the demos module, we got a paths object that has two methods. The <font color='firebrick'><i><b>available</b></i></font> method lists all the datasets available in the local demos/data directory as well as the demo data available in a remote Zenodo repository.The <font color='firebrick'><i><b>locate</b></i></font> method will return a local filepath to a named dataset. To do this, locate may need to download the data first depending on whether the data file is already on your system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e54e0658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Available demo data files & location---\n",
      "------------------------------------------\n",
      "annotations_001.txt            '/home/matt/python...nnotations_001.txt'\n",
      "recording_001.edf              '/home/matt/python.../recording_001.edf'\n",
      "5872_Left_group A.txt          '/home/matt/python...2_Left_group A.txt'\n",
      "split0.edf                     '/home/matt/python...os/data/split0.edf'\n",
      "5872_Left_group A-D.edf        '/home/matt/python...Left_group A-D.edf'\n",
      "irregular_write_test.edf       '/home/matt/python...lar_write_test.edf'\n",
      "write_test.edf                 '/home/matt/python...ata/write_test.edf'\n",
      "CW0259_SWDs.npy                '/home/matt/python...ta/CW0259_SWDs.npy'\n",
      "subset_001.edf                 '/home/matt/python...ata/subset_001.edf'\n",
      "split1.edf                     '/home/matt/python...os/data/split1.edf'\n"
     ]
    }
   ],
   "source": [
    "# check out the available demo data including with openseize\n",
    "# this will include any local data in demos/data and remote data stored at openseizes Zenodo repository.\n",
    "demos.paths.available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1a8af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not in on your machine, a confirmation box will open to confirm your download\n",
    "recording_path = demos.paths.locate('recording_001.edf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82f90652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Available demo data files & location---\n",
      "------------------------------------------\n",
      "annotations_001.txt            '/home/matt/python...nnotations_001.txt'\n",
      "recording_001.edf              '/home/matt/python.../recording_001.edf'\n",
      "5872_Left_group A.txt          '/home/matt/python...2_Left_group A.txt'\n",
      "split0.edf                     '/home/matt/python...os/data/split0.edf'\n",
      "5872_Left_group A-D.edf        '/home/matt/python...Left_group A-D.edf'\n",
      "irregular_write_test.edf       '/home/matt/python...lar_write_test.edf'\n",
      "write_test.edf                 '/home/matt/python...ata/write_test.edf'\n",
      "CW0259_SWDs.npy                '/home/matt/python...ta/CW0259_SWDs.npy'\n",
      "subset_001.edf                 '/home/matt/python...ata/subset_001.edf'\n",
      "split1.edf                     '/home/matt/python...os/data/split1.edf'\n"
     ]
    }
   ],
   "source": [
    "# now confirm the recording_001.edf file is on your machine\n",
    "demos.paths.available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce85d4e3",
   "metadata": {},
   "source": [
    "#### Building Data Readers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca468bb1",
   "metadata": {},
   "source": [
    "<font size=3>Ok, so we have a path to a data file. <font color='darkcyan'><b>So can we make a producer using this path?</b></font> No, openseize is a highly extensible package that will support many  different file types. Each of these file types will need to be read according to its own protocol. For reading EDF files, openseize has an EDF reader that reads... well EDF files. We discuss file readers in the file_reading demo. For now, we will make the EDF reader and explain the steps as we go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfe8edd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Reader in module openseize.file_io.edf:\n",
      "\n",
      "class Reader(openseize.file_io.bases.Reader)\n",
      " |  Reader(path: Union[str, pathlib.Path]) -> None\n",
      " |  \n",
      " |  A reader of European Data Format (EDF/EDF+) files.\n",
      " |  \n",
      " |  This reader supports reading EEG data and metadata from an EDF file with\n",
      " |  and without context management (see Introduction). If opened outside\n",
      " |  of context management, you should close this Reader's instance manually\n",
      " |  by calling the 'close' method to recover open file resources when you\n",
      " |  finish processing a file.\n",
      " |  \n",
      " |  Attributes:\n",
      " |      header (dict):\n",
      " |          A dictionary representation of the EDFs header.\n",
      " |      shape (tuple):\n",
      " |          A (channels, samples) shape tuple.\n",
      " |      channels (Sequence):\n",
      " |          The channels to be returned from the 'read' method call.\n",
      " |  \n",
      " |  Examples:\n",
      " |      >>> from openseize.demos import paths\n",
      " |      >>> filepath = paths.locate('recording_001.edf')\n",
      " |      >>> from openseize.io.edf import Reader\n",
      " |      >>> # open a reader using context management and reading 120 samples\n",
      " |      >>> # from all 4 channels\n",
      " |      >>> with Reader(filepath) as infile:\n",
      " |      >>>     x = infile.read(start=0, stop=120)\n",
      " |      >>> print(x.shape)\n",
      " |      ... (4, 120)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Reader\n",
      " |      openseize.file_io.bases.Reader\n",
      " |      abc.ABC\n",
      " |      openseize.core.mixins.ViewInstance\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, path: Union[str, pathlib.Path]) -> None\n",
      " |      Extends the Reader ABC with a header attribute.\n",
      " |  \n",
      " |  read(self, start: int, stop: Optional[int] = None, padvalue: float = nan) -> numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]]\n",
      " |      Reads samples from this EDF from this Reader's channels.\n",
      " |      \n",
      " |      Args:\n",
      " |          start:\n",
      " |              The start sample index to read.\n",
      " |          stop:\n",
      " |              The stop sample index to read (exclusive). If None, samples\n",
      " |              will be read until the end of file.\n",
      " |          padvalue:\n",
      " |              Value to pad to channels that run out of samples to return.\n",
      " |              Only applicable if sample rates of channels differ.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A float64 array of shape len(chs) x (stop-start) samples.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  shape\n",
      " |      Returns a 2-tuple containing the number of channels and\n",
      " |      number of samples in this EDF.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  channels\n",
      " |      Returns the channels that this Reader will read.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from openseize.file_io.bases.Reader:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Return reader instance as target variable of this context.\n",
      " |  \n",
      " |  __exit__(self, exc_type, exc_value, traceback)\n",
      " |      On context exit, close this reader's file object and propagate\n",
      " |      errors by returning None.\n",
      " |  \n",
      " |  close(self)\n",
      " |      Close this reader instance's opened file object and destroy the\n",
      " |      reference to the file object.\n",
      " |      \n",
      " |      File descriptors whether opened or closed are not serializable. To\n",
      " |      support concurrent processing we close & remove all references to the\n",
      " |      file descriptor on close.\n",
      " |  \n",
      " |  open(self)\n",
      " |      Opens the file at path for reading & stores the file descriptor to\n",
      " |      this Reader's '_fobj' attribute.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from openseize.file_io.bases.Reader:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from openseize.core.mixins.ViewInstance:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Returns the __init__'s signature as the echo representation.\n",
      " |      \n",
      " |      Returns: str\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Returns this instances print representation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how do  we build an edf reader -- ask for help!\n",
    "help(edf.Reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40c71a0",
   "metadata": {},
   "source": [
    "<font size=3>Under the 'Methods' section we see that to make a reader all we need is an edf path.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ded6b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the reader using the recording path we fetched\n",
    "reader = edf.Reader(recording_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890cc5ed",
   "metadata": {},
   "source": [
    "<font size=3>Also notice under the methods there is just one method called <i>read</i>. It reads samples from the EDF file between start and stop sample indices for each channel in channels list. Lets see if this does something.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4eb9aaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-19.87908032   7.95793213  19.88808032  18.89390131  18.89390131\n",
      "   43.74837671   6.96375311  30.8240495    5.9695741   22.87061737]\n",
      " [-86.4890744   51.70180884  63.63195703  88.48643243  63.63195703\n",
      "   61.643599    54.68434589  43.74837671  55.6785249   64.62613605]\n",
      " [-85.49489539  44.74255573  29.82987048  79.53882129  52.69598785\n",
      "   42.75419769  42.75419769  21.87643835  60.64941998  66.61449408]\n",
      " [ 62.63777802  95.44568555  77.55046326  36.7891236  109.36419177\n",
      "  118.31180292 122.28851898 115.32926587  76.55628424  44.74255573]]\n"
     ]
    }
   ],
   "source": [
    "values = reader.read(start=0, stop=10) #channels unspecified means read all channels\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111530f5",
   "metadata": {},
   "source": [
    "<font size=3>So we got an array of shape (4,10) back. <font color='darkcyan'><b><i>Is that right?</b></i></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "161efa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reader Object\n",
      "---Attributes & Properties---\n",
      "{'path': PosixPath('/home/matt/python/nri/openseize/src/openseize/demos/data/recording_001.edf'),\n",
      " 'mode': 'rb',\n",
      " 'kwargs': {},\n",
      " 'header': {'version': '0',\n",
      "            'patient': 'PIN-42 M 11-MAR-1952 Animal',\n",
      "            'recording': 'Startdate 15-AUG-2020 X X X',\n",
      "            'start_date': '15.08.20',\n",
      "            'start_time': '09.59.15',\n",
      "            'header_bytes': 1536,\n",
      "            'reserved_0': 'EDF+C',\n",
      "            'num_records': 3775,\n",
      "            'record_duration': 1.0,\n",
      "            'num_signals': 5,\n",
      "            'names': ['EEG EEG_1_SA-B', 'EEG EEG_2_SA-B', 'EEG EEG_3_SA-B',\n",
      "                      'EEG EEG_4_SA-B', 'EDF Annotations'],\n",
      "            'transducers': ['8401 HS:15279', '8401 HS:15279', '8401 HS:15279',\n",
      "                            '8401 HS:15279', ''],\n",
      "            'physical_dim': ['uV', 'uV', 'uV', 'uV', ''],\n",
      "            'physical_min': [-8144.31, -8144.31, -8144.31, -8144.31, -1.0],\n",
      "            'physical_max': [8144.319, 8144.319, 8144.319, 8144.319, 1.0],\n",
      "            'digital_min': [-8192.0, -8192.0, -8192.0, -8192.0, -32768.0],\n",
      "            'digital_max': [8192.0, 8192.0, 8192.0, 8192.0, 32767.0],\n",
      "            'prefiltering': ['none', 'none', 'none', 'none', ''],\n",
      "            'samples_per_record': [5000, 5000, 5000, 5000, 1024],\n",
      "            'reserved_1': ['', '', '', '', '']},\n",
      " 'channels': [0, 1, 2, 3],\n",
      " 'shape': (4, 18875000)}\n",
      "\n",
      "Type help(Reader) for full documentation\n"
     ]
    }
   ],
   "source": [
    "# examine the reader with print\n",
    "print(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6b7ef5",
   "metadata": {},
   "source": [
    "<font size=3>Awesome! Printing the reader gives us the EDFs header which tells us everything we need to know. The file contains 4 channels named [EEG EEG_1_SA-B,... EEG EEG_4_SA-B]. So our array shape makes sense.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d224c206",
   "metadata": {},
   "source": [
    "<font size=3><font color='darkcyan'><b>Can we finally build a producer? YES</b></font>, producers can produce from any reader type as long as the reader has a method called <font color='firebrick'><b>read</b></font>. Lucky for us the EDF reader we just built has a read method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68cd031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a producer from our edf reader instance\n",
    "# the  chunksize will  be set to 100k samples\n",
    "# the axis should be 1 since the reader has samples along this axis\n",
    "rpro = producer(reader, chunksize=100000, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78e575be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReaderProducer Object\n",
      "---Attributes & Properties---\n",
      "{'data': Reader(path: Union[str, pathlib.Path]) -> None,\n",
      " 'axis': 1,\n",
      " 'kwargs': {},\n",
      " 'chunksize': 100000,\n",
      " 'shape': (4, 18875000)}\n",
      "\n",
      "Type help(ReaderProducer) for full documentation\n"
     ]
    }
   ],
   "source": [
    "# lets check out the data and attributes of this producer\n",
    "print(rpro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46a34705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pro is using = 48 Bytes\n"
     ]
    }
   ],
   "source": [
    "# and lets checkout the producer's memory consumption\n",
    "print('pro is using = {} Bytes'.format(sys.getsizeof(pro)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f326946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data would be = 604.0 MB\n"
     ]
    }
   ],
   "source": [
    "# how much memory would we use if we loaded all the data in at once?\n",
    "size = np.multiply(*rpro.shape)\n",
    "itemsize = 8 #8 bytes in a float64\n",
    "print('data would be = {} MB'.format((size * itemsize)/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acd08dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 100000), (4, 75000)]\n"
     ]
    }
   ],
   "source": [
    "# lets also get the shape of each produced array\n",
    "shapes = [arr.shape for arr in rpro]\n",
    "print(shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ff7089",
   "metadata": {},
   "source": [
    "<font size=3>So this ReaderProducer is giving us access to a little over 600 MB of data using only 48 bytes!\n",
    "<b>Lets  test to make sure the produced values equal the values from the file read in as a single array.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d11ff89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do they match... True\n"
     ]
    }
   ],
   "source": [
    "# This will consume 604 MB of data\n",
    "produced_array = rpro.to_array()\n",
    "\n",
    "# test if this matches the array from reading all the data\n",
    "read_array = reader.read(0, stop=None) #if stop is None the reader reads to the end of file\n",
    "\n",
    "print('Do they match...', np.allclose(produced_array, read_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b1e20f",
   "metadata": {},
   "source": [
    "### Masked Producers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b7b747",
   "metadata": {},
   "source": [
    "#### Masking with Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a746fe77",
   "metadata": {},
   "source": [
    "<font size=3>While the producer will produce sequential chunks of data from the start of file to the end of a file, it is common that researchers only want to analyze specific sections of the data. To support non-contiguous production of arrays from a data source, producers can be initialized with a mask. <b>Let's see how to use an annotation file to create a mask so that the producer only produces data we want.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ab257e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Available demo data files & location---\n",
      "------------------------------------------\n",
      "annotations_001.txt            '/home/matt/python...nnotations_001.txt'\n",
      "recording_001.edf              '/home/matt/python.../recording_001.edf'\n",
      "5872_Left_group A.txt          '/home/matt/python...2_Left_group A.txt'\n",
      "split0.edf                     '/home/matt/python...os/data/split0.edf'\n",
      "5872_Left_group A-D.edf        '/home/matt/python...Left_group A-D.edf'\n",
      "irregular_write_test.edf       '/home/matt/python...lar_write_test.edf'\n",
      "write_test.edf                 '/home/matt/python...ata/write_test.edf'\n",
      "CW0259_SWDs.npy                '/home/matt/python...ta/CW0259_SWDs.npy'\n",
      "subset_001.edf                 '/home/matt/python...ata/subset_001.edf'\n",
      "split1.edf                     '/home/matt/python...os/data/split1.edf'\n"
     ]
    }
   ],
   "source": [
    "# again lets check the available demo data\n",
    "demos.paths.available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f8068df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets fetch the annotations_001.edf. This contains user annotations for recording_001.edf\n",
    "filepath = demos.paths.locate('annotations_001.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61a9f8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Experiment ID\tExperiment\n",
      "\n",
      "1 Animal ID\tAnimal\n",
      "\n",
      "2 Researcher\tTest\n",
      "\n",
      "3 Directory path\t\n",
      "\n",
      "4 \n",
      "\n",
      "5 \n",
      "\n",
      "6 Number\tStart Time\tEnd Time\tTime From Start\tChannel\tAnnotation\n",
      "\n",
      "7 0\t08/15/20 09:59:15.215\t08/15/20 09:59:15.215\t0.0000\tALL\tStarted Recording\n",
      "\n",
      "8 1\t08/15/20 10:00:00.000\t08/15/20 10:00:00.000\t44.7850\tALL\tQi_start\n",
      "\n",
      "9 2\t08/15/20 10:00:25.000\t08/15/20 10:00:30.000\t69.7850\tALL\tgrooming\n",
      "\n",
      "10 3\t08/15/20 10:00:45.000\t08/15/20 10:00:50.000\t89.7850\tALL\tgrooming\n",
      "\n",
      "11 4\t08/15/20 10:02:15.000\t08/15/20 10:02:20.000\t179.7850\tALL\tgrooming\n",
      "\n",
      "12 5\t08/15/20 10:04:36.000\t08/15/20 10:04:41.000\t320.7850\tALL\texploring\n",
      "\n",
      "13 6\t08/15/20 10:05:50.000\t08/15/20 10:05:55.000\t394.7850\tALL\texploring\n",
      "\n",
      "14 7\t08/15/20 10:08:50.000\t08/15/20 10:08:55.000\t574.7850\tALL\trest\n",
      "\n",
      "15 8\t08/15/20 10:10:14.000\t08/15/20 10:10:19.000\t658.7850\tALL\texploring\n",
      "\n",
      "16 9\t08/15/20 10:17:10.000\t08/15/20 10:17:15.000\t1074.7850\tALL\trest\n",
      "\n",
      "17 10\t08/15/20 10:35:49.000\t08/15/20 10:35:54.000\t2193.7850\tALL\trest\n",
      "\n",
      "18 11\t08/15/20 10:40:00.000\t08/15/20 10:40:00.000\t2444.7850\tALL\tQi_stop\n",
      "\n",
      "19 12\t08/15/20 11:02:09.879\t08/15/20 11:02:09.879\t3774.6640\tALL\tStopped Recording\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets have a look at this file's contents\n",
    "with open(filepath, 'r') as infile:\n",
    "    for idx, line in enumerate(infile):\n",
    "        print(idx, line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdbfee1",
   "metadata": {},
   "source": [
    "<font  size=3>This file contains 13 annotations; these annotations include user start and stop, rest, grooming and exploring. Our goal is to use these annotations to build a producer that produces values for only rest and exploring times.</font>\n",
    "\n",
    "<font  size=3>Openseize has a set of annotation file readers. This file is in the Pinnacle format so we will use the Pinnacle annotations reader (this is described further in the file_reading demo).\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7fcdad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation(label='exploring', time=320.785, duration=5.0, channel='ALL')\n",
      "Annotation(label='exploring', time=394.785, duration=5.0, channel='ALL')\n",
      "Annotation(label='rest', time=574.785, duration=5.0, channel='ALL')\n",
      "Annotation(label='exploring', time=658.785, duration=5.0, channel='ALL')\n",
      "Annotation(label='rest', time=1074.785, duration=5.0, channel='ALL')\n",
      "Annotation(label='rest', time=2193.785, duration=5.0, channel='ALL')\n"
     ]
    }
   ],
   "source": [
    "# for details on reading Pinnacle files see the file_reading demo\n",
    "\n",
    "# Pinnacle files need a path & the column header line (start)\n",
    "with annotations.Pinnacle(path=filepath, start=6) as areader:\n",
    "    # read creates a list of annotation dataclass instances that contain the label, \n",
    "    # time (in secs), duration (in secs) and the channel(s) the annotation was marked \n",
    "    # for.\n",
    "    annotes = areader.read(labels=['rest','exploring'])\n",
    "\n",
    "# lets print each annotation instance in the read_annotes list\n",
    "for an in annotes:\n",
    "    print(an)\n",
    "    \n",
    "#you can see the times match the time from the start of recording in the text file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7be91f",
   "metadata": {},
   "source": [
    "<font size=3 color='darkcyan'><b>So how do we take these Annotation dataclass instances and turn them into a mask for a producer?</b></font>\n",
    "\n",
    "The annotations module in openseize has a function called <font color='firebrick'><i>as_mask</i></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2c2dec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function as_mask in module openseize.file_io.annotations:\n",
      "\n",
      "as_mask(annotations: Sequence[openseize.file_io.bases.Annotation], size: int, fs: float, include: bool = True) -> numpy.ndarray[typing.Any, numpy.dtype[numpy.bool_]]\n",
      "    Creates a boolean mask from a sequence of annotation dataclass\n",
      "    instances..\n",
      "    \n",
      "    Producers of EEG data may receive an optional boolean array mask.  This\n",
      "    function creates a boolean mask from a sequence of annotations and is\n",
      "    therefore useful for filtering EEG data by annotation label during\n",
      "    processing.\n",
      "    \n",
      "    Args:\n",
      "        annotations:\n",
      "            A sequence of annotation dataclass instances to convert to a \n",
      "            mask.\n",
      "        size:\n",
      "            The length of the boolean array to return.\n",
      "        fs:\n",
      "            The sampling rate in Hz of the digital system.\n",
      "        include:\n",
      "            Boolean determining if annotations should be set to True or\n",
      "            False in the returned array. True means all values\n",
      "            are False in the returned array except for samples where the\n",
      "            annotations are located.\n",
      "    \n",
      "    Returns:\n",
      "        A 1-D boolean array of length size.\n",
      "    \n",
      "    Examples:\n",
      "        >>> # read the annotations from the demo annotation file\n",
      "        >>> from openseize.demos import paths\n",
      "        >>> filepath = paths.locate('annotations_001.txt')\n",
      "        >>> from openseize.io.annotations import Pinnacle\n",
      "        >>> # read the 'rest' annotations\n",
      "        >>> with Pinnacle(filepath, start=6) as pinnacle:\n",
      "        >>>     annotations = pinnacle.read(labels=['rest'])\n",
      "        >>> # create a mask measuring 3700 secs at 5000 Hz\n",
      "        >>> mask = as_mask(annotations, size=3700*5000, fs=5000)\n",
      "        >>> # measure the total time in secs of 'rest' annotation\n",
      "        >>> print(np.count_nonzero(mask) / 5000)\n",
      "        15.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(annotations.as_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bfdf28",
   "metadata": {},
   "source": [
    "<font size=3>To make a mask we need the annotation dataclasses, the length of the mask, and the sampling rate. The sampling rate is needed to convert the time of the annotation into a sample number.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5bef3277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18875000,)\n"
     ]
    }
   ],
   "source": [
    "# make a mask the same size as the number of samples in the reader & set values at the annotes to True\n",
    "mask = annotations.as_mask(annotes, size=reader.shape[-1], fs=5000, include=True)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0088195f",
   "metadata": {},
   "source": [
    "<font size=3>Note that the size of the mask is the same size as all the samples in the reader. So now we are ready to make a Masked Producer.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de4b87a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_pro = producer(reader, chunksize=100000, axis=-1, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3bedaafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaskedProducer Object\n",
      "---Attributes & Properties---\n",
      "{'data': ReaderProducer(data, chunksize, axis, **kwargs),\n",
      " 'axis': -1,\n",
      " 'kwargs': {},\n",
      " 'mask': ArrayProducer(data, chunksize, axis, **kwargs),\n",
      " 'chunksize': 100000,\n",
      " 'shape': (4, 150000)}\n",
      "\n",
      "Type help(MaskedProducer) for full documentation\n"
     ]
    }
   ],
   "source": [
    "print(masked_pro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a75e7f5",
   "metadata": {},
   "source": [
    "<font size=3>Notice the size of the masked producer is now 150K. This is because we are keeping rest and exploring periods of the EEG which amounts to 30 secs of data @ 5 KHz = 150K samples</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ff40636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 100000), (4, 50000)]\n"
     ]
    }
   ],
   "source": [
    "# as before, we can loop over the producer with a mask \n",
    "# this will give us the shape of the produced arrays ONLY during periods of rest and exploring\n",
    "shapes = [arr.shape for arr in masked_pro]\n",
    "print(shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22773c56",
   "metadata": {},
   "source": [
    "<font size=3>In the above example we built a mask from a list of annotation dataclass instances but a masked producer can be built using any numpy boolean array. This allows for spohisticated masking conditions. For example to mask between certain hours of the EEG and filter out periods like grooming, you can  construct 2 mask <i>and</i> take their intersection as the mask to apply to the producer.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9996f669",
   "metadata": {},
   "source": [
    "### Producers from Generating functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d08477",
   "metadata": {},
   "source": [
    "<font size=3>A major question that developers of Openseize needed to address is <font color='darkcyan'><b>What good is a producer if you have to convert it into an array in order to compute something?</b></font>\n",
    "\n",
    "<font size=3>To address this, Openseize relies on converting generating functions into producers. This allows for computations to be computed on-the-fly inside the body of a generating function and converted into a multitransversal producer iterable. This is a lot to take in so we are going to go through a few simple examples.\n",
    "  \n",
    "<font size=3>To wield openseize taking advantage of its high memory effeciency, you'll need to familarize yourself with generating functions: <a href=https://realpython.com/introduction-to-python-generators/>generating functions tutorial</a></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8db9e955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReaderProducer Object\n",
      "---Attributes & Properties---\n",
      "{'data': Reader(path: Union[str, pathlib.Path]) -> None,\n",
      " 'axis': 1,\n",
      " 'kwargs': {},\n",
      " 'chunksize': 100000,\n",
      " 'shape': (4, 18875000)}\n",
      "\n",
      "Type help(ReaderProducer) for full documentation\n"
     ]
    }
   ],
   "source": [
    "# lets start by re-examining our producer built from the values in the edf file\n",
    "print(rpro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455726c2",
   "metadata": {},
   "source": [
    "<font size=3> Now lets say that we want to take every value in this producer and square it. To do this we will need to loop over the producer but we don't want to store all the squared values to an array since that take a lot of memory. The solution is to build a generating function.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18f1a7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared(pro):\n",
    "    \"\"\"A generating function that yields the squared values of each subarray in a producer.\"\"\"\n",
    "    \n",
    "    for arr in pro:\n",
    "        yield arr**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f7766c",
   "metadata": {},
   "source": [
    "<font size=3>Now lets make our generator and  run it. We will compute the largest squared value in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0bd47def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest squared value of the 189 arrays is 1061277445.6032624\n"
     ]
    }
   ],
   "source": [
    "# compute the max squared value across all chunks\n",
    "gen = squared(rpro) #make a generator\n",
    "\n",
    "extreme = 0\n",
    "cnt = 0\n",
    "for arr in gen:\n",
    "    \n",
    "    # max of this sub arr\n",
    "    m = np.max(arr)\n",
    "    cnt += 1\n",
    "    \n",
    "    if m > extreme:\n",
    "        extreme = m\n",
    "\n",
    "print('The largest squared value of the {} arrays is {}'.format(cnt, extreme))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e68387",
   "metadata": {},
   "source": [
    "<font size=3>The problem with this approach is that the generator 'gen' has now been exhausted. So anytime we need the square values we have to make a new generator from the generating function. To see this try to get the generator's next value...</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40cf517b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't do it-- this gen is empty\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    next(gen)\n",
    "\n",
    "except StopIteration:\n",
    "    print (\"Can't do it-- this gen is empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16189471",
   "metadata": {},
   "source": [
    "<font size=3>Another problem is that the generator can't be copied. When you make a copy (called teeing) and advance one generator, you advance all copies of the generator. This is a huge problem because we need to make copies in order to advance generators independently to solve problems where the computed value at a sample depends on the surrounding sample values.</font>\n",
    "\n",
    "<font size=3><font color='darkcyan'><b>The way out of these problems is to make producers from generators. Lets go through this.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0fc006c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make a producer of squared values using the squared generating function\n",
    "\n",
    "# notice we pass in the generating function, a chunksize, an axis and the shape of all arrays \n",
    "# that will be yielded by the generating function. Since it just squares values, the shape is the \n",
    "# same as the rpro shape. Lastly squared is a function and it needs a producer so we pass that as a\n",
    "# keyword argument\n",
    "\n",
    "squared_pro = producer(squared, chunksize=120000, axis=-1, shape=rpro.shape, pro=rpro)\n",
    "\n",
    "# lastly notice that we gave a chunksize 120k that was different that rpro chunksize = 100k. That's ok, \n",
    "# producers are smart enough to figure out how to collect values from the generating function until the \n",
    "# new chunksize is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "006cd0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest squared value of the 158 arrays is 1061277445.6032624\n"
     ]
    }
   ],
   "source": [
    "# lets again compute the maximum squared value\n",
    "\n",
    "extreme = 0\n",
    "cnt = 0\n",
    "for arr in squared_pro:\n",
    "    \n",
    "    # max of this sub arr\n",
    "    m = np.max(arr)\n",
    "    cnt += 1\n",
    "    \n",
    "    if m > extreme:\n",
    "        extreme = m\n",
    "        \n",
    "# the number of arrays will be less b/c we increased chunksize to 120k\n",
    "print('The largest squared value of the {} arrays is {}'.format(cnt, extreme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "64aa8666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squared_pro has 158 subarrays.\n"
     ]
    }
   ],
   "source": [
    "# since squared_pro is a producer, it is multitransversal\n",
    "cnt = 0\n",
    "for sub_arr in squared_pro:\n",
    "    cnt += 1\n",
    "\n",
    "print('squared_pro has {} subarrays.'.format(cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "82382fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a copy of the producer is easy because openseize supports producer creation from producers.\n",
    "squared_pro2 = producer(squared_pro, chunksize=10000, axis=-1) #These move independently from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5916c361",
   "metadata": {},
   "source": [
    "<font size=3>To recap, we can convert generating functions into producers. These producers are both multitransversal and support copying, features that are absent from generators. This makes them far more usable in numerical code.\n",
    "\n",
    "<font size=3>To simplify creating producers from generating functions, Openseize uses a decorator called as_producer. This decorator can turn generating functions into producers without the need to explicitly call the producer creator \"producer(...)\". <b>Lets see this in action.</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "58633a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#again we will make a squaring producer but this time using the as_producer decorator\n",
    "\n",
    "@as_producer\n",
    "def square_deco(pro):\n",
    "    for arr in pro:\n",
    "        yield arr**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a7d7c62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest squared value of the 189 subarrays is 1061277445.6032624\n"
     ]
    }
   ],
   "source": [
    "# lets again compute the maximum squared value\n",
    "\n",
    "extreme = 0\n",
    "cnt = 0\n",
    "for arr in square_deco(rpro):\n",
    "    \n",
    "    # max of this sub arr\n",
    "    m = np.max(arr)\n",
    "    cnt += 1\n",
    "    \n",
    "    if m > extreme:\n",
    "        extreme = m\n",
    "\n",
    "print('The largest squared value of the {} subarrays is {}'.format(cnt, extreme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aa4fd80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squared_pro has 189 subarrays.\n",
      "We excpected 189 subarrays\n"
     ]
    }
   ],
   "source": [
    "# since squared_pro is a producer it is multitransversal\n",
    "cnt = 0\n",
    "for sub_arr in square_deco(rpro):\n",
    "    cnt += 1\n",
    "\n",
    "print('squared_pro has {} subarrays.'.format(cnt))\n",
    "\n",
    "# Did we get the right number of subarrays\n",
    "samples = rpro.shape[-1]\n",
    "csize = rpro.chunksize\n",
    "print('We excpected {} subarrays'.format(samples // csize + bool(samples % csize)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea86d630",
   "metadata": {},
   "source": [
    "<font size=3>This section brought us into the deep topic of converting generating functions into producers. Not all users will need this. Openseize provides many tools that you can just call like a regular function. Just know that under-the-hood, these functions are using this conversion. If you need to compute quantities that are not part of openseize and your data is too large for memory, then creating producers from generating functions is the way to go.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ab73c6",
   "metadata": {},
   "source": [
    "### Producers From Producers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9db87ed",
   "metadata": {},
   "source": [
    "<font size=3> In the last section, we noted that you can copy producers and then use them independently. The construction of producer copies is straightforward.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a58ea55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Producer\n",
      " ReaderProducer Object\n",
      "---Attributes & Properties---\n",
      "{'data': Reader(path: Union[str, pathlib.Path]) -> None,\n",
      " 'axis': 1,\n",
      " 'kwargs': {},\n",
      " 'chunksize': 100000,\n",
      " 'shape': (4, 18875000)}\n",
      "\n",
      "Type help(ReaderProducer) for full documentation\n",
      "\n",
      "\n",
      "New Producer\n",
      " ReaderProducer Object\n",
      "---Attributes & Properties---\n",
      "{'data': Reader(path: Union[str, pathlib.Path]) -> None,\n",
      " 'axis': -1,\n",
      " 'kwargs': {},\n",
      " 'chunksize': 150000,\n",
      " 'shape': (4, 18875000)}\n",
      "\n",
      "Type help(ReaderProducer) for full documentation\n"
     ]
    }
   ],
   "source": [
    "# print our EDF data producer\n",
    "print('Original Producer\\n',rpro)\n",
    "print('\\n')\n",
    "# make a new producer changing the chunksize\n",
    "newpro = producer(rpro, chunksize=150000, axis=-1)\n",
    "print('New Producer\\n', newpro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6289a30c",
   "metadata": {},
   "source": [
    "<font size=3>These producers can be iterated over independently.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ca616b",
   "metadata": {},
   "source": [
    "## Resource Recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c7ce0b",
   "metadata": {},
   "source": [
    "<font size=3>Throughout the last 1/2 of this tutorial, we have used a reader instance. This instance uses resources to provide access to the opened edf file. It is important to close these instances when you are done to recover those resources. In the file-reading demo, we will show how to create a reader using a <font color='firebrick'>Context Manager</font> which will automatically close the file when you are done with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fc970a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the open reader instance\n",
    "reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4579b8a",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db67b97e",
   "metadata": {},
   "source": [
    "<font size=3><b>Producers are at the heart of openseize.</b> All available methods in the modules of openseize can accept producer of arrays. This means openseize can compute quantities even when the input or output may not fit into virtual memory. Your data may not require this level of memory effeciency. If not, you can also supply ndarrays into openseize methods. Be aware that some methods may run faster using producer inputs and some may run faster using full arrays. The goal here is to give users options for how they want to use their machine's memory. Lastly, chunksizes can also effect the speed of computations. There is a goldilock's zone that will depend on the available memory of your machine. Openseize gives you the flexibility to adjust this parameter so you decide how much memory you want a process to take.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
