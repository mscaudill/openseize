{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1d5c4bf",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab17e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from openseize.file_io.bases import Reader, Header, Writer\n",
    "from openseize.file_io import edf, annotations\n",
    "from openseize import demos\n",
    "from openseize import producer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2d6827",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5a9e71",
   "metadata": {},
   "source": [
    "<font size=3>Openseize currently provides tools for reading and writing European Data Format (EDF) binary files. The details of this file specification can be found here: https://www.edfplus.info/specs/edf.html\n",
    "\n",
    "<font size=3>This demo will describe how to open, read, and produce data from an EDF file using the <font color='darkcyan'><b>EDF Reader class</b></font> and write data to an EDF file using the <font color='darkcyan'><b>EDF Writer class</b></font>. Additionally, this demo will cover how to read Comma Separated (CSV) and Tab separated value (TSV) annotation text files and use the resulting annotations to mask produced EEG numpy arrays.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a718b1f2",
   "metadata": {},
   "source": [
    "## Reading EDF Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048d682b",
   "metadata": {},
   "source": [
    "<font size=3>Our goal is to use a Reader object to read from an EDF file. Let's first take a look at the help file available for the Reader class, to see what we'll need in order to create one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27e09623",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Reader in module openseize.file_io.edf:\n",
      "\n",
      "class Reader(openseize.file_io.bases.Reader)\n",
      " |  Reader(path: Union[str, pathlib.Path]) -> None\n",
      " |  \n",
      " |  A reader of European Data Format (EDF/EDF+) files.\n",
      " |  \n",
      " |  This reader supports reading EEG data and metadata from an EDF file with\n",
      " |  and without context management (see Introduction). If opened outside\n",
      " |  of context management, you should close this Reader's instance manually\n",
      " |  by calling the 'close' method to recover open file resources when you\n",
      " |  finish processing a file.\n",
      " |  \n",
      " |  Attributes:\n",
      " |      header (dict):\n",
      " |          A dictionary representation of the EDFs header.\n",
      " |      shape (tuple):\n",
      " |          A (channels, samples) shape tuple.\n",
      " |      channels (Sequence):\n",
      " |          The channels to be returned from the 'read' method call.\n",
      " |  \n",
      " |  Examples:\n",
      " |      >>> from openseize.demos import paths\n",
      " |      >>> filepath = paths.locate('recording_001.edf')\n",
      " |      >>> from openseize.io.edf import Reader\n",
      " |      >>> # open a reader using context management and reading 120 samples\n",
      " |      >>> # from all 4 channels\n",
      " |      >>> with Reader(filepath) as infile:\n",
      " |      >>>     x = infile.read(start=0, stop=120)\n",
      " |      >>> print(x.shape)\n",
      " |      ... (4, 120)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Reader\n",
      " |      openseize.file_io.bases.Reader\n",
      " |      abc.ABC\n",
      " |      openseize.core.mixins.ViewInstance\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, path: Union[str, pathlib.Path]) -> None\n",
      " |      Extends the Reader ABC with a header attribute.\n",
      " |  \n",
      " |  read(self, start: int, stop: Optional[int] = None, padvalue: float = nan) -> numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]]\n",
      " |      Reads samples from this EDF from this Reader's channels.\n",
      " |      \n",
      " |      Args:\n",
      " |          start:\n",
      " |              The start sample index to read.\n",
      " |          stop:\n",
      " |              The stop sample index to read (exclusive). If None, samples\n",
      " |              will be read until the end of file.\n",
      " |          padvalue:\n",
      " |              Value to pad to channels that run out of samples to return.\n",
      " |              Only applicable if sample rates of channels differ.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A float64 array of shape len(chs) x (stop-start) samples.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  shape\n",
      " |      Returns a 2-tuple containing the number of channels and\n",
      " |      number of samples in this EDF.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  channels\n",
      " |      Returns the channels that this Reader will read.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from openseize.file_io.bases.Reader:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Return reader instance as target variable of this context.\n",
      " |  \n",
      " |  __exit__(self, exc_type, exc_value, traceback)\n",
      " |      On context exit, close this reader's file object and propogate\n",
      " |      errors by returning None.\n",
      " |  \n",
      " |  close(self)\n",
      " |      Close this reader instance's opened file object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from openseize.file_io.bases.Reader:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from openseize.core.mixins.ViewInstance:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Returns the __init__'s signature as the echo representation.\n",
      " |      \n",
      " |      Returns: str\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Returns this instances print representation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(edf.Reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faae001c",
   "metadata": {},
   "source": [
    "<font size=3>The only parameter that is required to instantiate the Reader is a path to an EDF file. So we should pick a file to read off of. For these demos, we have stored demo data to a remote Zenodo repository. The demos module we imported has access to the files in this repo; we can see what's available by calling the <font color='firebrick'><i><b>available</b></i></font> method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29c59cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Available demo data files & location---\n",
      "------------------------------------------\n",
      "annotations_001.txt            '/home/matt/python...nnotations_001.txt'\n",
      "recording_001.edf              '/home/matt/python.../recording_001.edf'\n",
      "5872_Left_group A.txt          '/home/matt/python...2_Left_group A.txt'\n",
      "split0.edf                     '/home/matt/python...os/data/split0.edf'\n",
      "5872_Left_group A-D.edf        '/home/matt/python...Left_group A-D.edf'\n",
      "irregular_write_test.edf       '/home/matt/python...lar_write_test.edf'\n",
      "write_test.edf                 '/home/matt/python...ata/write_test.edf'\n",
      "CW0259_SWDs.npy                '/home/matt/python...ta/CW0259_SWDs.npy'\n",
      "split1.edf                     '/home/matt/python...os/data/split1.edf'\n"
     ]
    }
   ],
   "source": [
    "demos.paths.available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a026c1",
   "metadata": {},
   "source": [
    "<font size=3> If the file is currently on your system, you'll see a local location after that file's name. If not, you'll see a link to the Zenodo repo. Regardless of its location, we can get access to a file by calling the <font color='firebrick'><i><b>locate</b></i></font> method. If the file hasn't already been found on your local machine, it will be downloaded to the demos data folder. This may take a few minutes, but will occur only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76597988",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get access to the file's path locally, downloading if needed\n",
    "filepath = demos.paths.locate('recording_001.edf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a364bc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Available demo data files & location---\n",
      "------------------------------------------\n",
      "annotations_001.txt            '/home/matt/python...nnotations_001.txt'\n",
      "recording_001.edf              '/home/matt/python.../recording_001.edf'\n",
      "5872_Left_group A.txt          '/home/matt/python...2_Left_group A.txt'\n",
      "split0.edf                     '/home/matt/python...os/data/split0.edf'\n",
      "5872_Left_group A-D.edf        '/home/matt/python...Left_group A-D.edf'\n",
      "irregular_write_test.edf       '/home/matt/python...lar_write_test.edf'\n",
      "write_test.edf                 '/home/matt/python...ata/write_test.edf'\n",
      "CW0259_SWDs.npy                '/home/matt/python...ta/CW0259_SWDs.npy'\n",
      "split1.edf                     '/home/matt/python...os/data/split1.edf'\n"
     ]
    }
   ],
   "source": [
    "# We can see the file's location on our local machine now that it has downloaded.\n",
    "demos.paths.available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc20745",
   "metadata": {},
   "source": [
    "<font size=3>Now that we have our demo file path, we can pass it in to create our Reader object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c714a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = edf.Reader(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad81790",
   "metadata": {},
   "source": [
    "### Properties and Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7160acaa",
   "metadata": {},
   "source": [
    "<font size=3>To view the attributes and properties of this reader we can print the reader instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b76b33d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reader Object\n",
      "---Attributes & Properties---\n",
      "{'path': PosixPath('/home/matt/python/nri/openseize/src/openseize/demos/data/recording_001.edf'),\n",
      " 'header': {'version': '0',\n",
      "            'patient': 'PIN-42 M 11-MAR-1952 Animal',\n",
      "            'recording': 'Startdate 15-AUG-2020 X X X',\n",
      "            'start_date': '15.08.20',\n",
      "            'start_time': '09.59.15',\n",
      "            'header_bytes': 1536,\n",
      "            'reserved_0': 'EDF+C',\n",
      "            'num_records': 3775,\n",
      "            'record_duration': 1.0,\n",
      "            'num_signals': 5,\n",
      "            'names': ['EEG EEG_1_SA-B', 'EEG EEG_2_SA-B', 'EEG EEG_3_SA-B',\n",
      "                      'EEG EEG_4_SA-B', 'EDF Annotations'],\n",
      "            'transducers': ['8401 HS:15279', '8401 HS:15279', '8401 HS:15279',\n",
      "                            '8401 HS:15279', ''],\n",
      "            'physical_dim': ['uV', 'uV', 'uV', 'uV', ''],\n",
      "            'physical_min': [-8144.31, -8144.31, -8144.31, -8144.31, -1.0],\n",
      "            'physical_max': [8144.319, 8144.319, 8144.319, 8144.319, 1.0],\n",
      "            'digital_min': [-8192.0, -8192.0, -8192.0, -8192.0, -32768.0],\n",
      "            'digital_max': [8192.0, 8192.0, 8192.0, 8192.0, 32767.0],\n",
      "            'prefiltering': ['none', 'none', 'none', 'none', ''],\n",
      "            'samples_per_record': [5000, 5000, 5000, 5000, 1024],\n",
      "            'reserved_1': ['', '', '', '', '']},\n",
      " 'channels': [0, 1, 2, 3],\n",
      " 'shape': (4, 18875000)}\n",
      "\n",
      "Type help(Reader) for full documentation\n"
     ]
    }
   ],
   "source": [
    "# Print out the reader object to see its attributes\n",
    "print(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f80bdb",
   "metadata": {},
   "source": [
    "<font size=3> The reader contains three attributes; a <font color=firebrick><b>path</b></font> to the open file, a dictionary containing the EDF's <font color=firebrick><b>header information</b></font>, and the <font color=firebrick><b>shape of the data</b></font>, represented as a 2-D numpy array, with channels along 0th axis and samples along the 1st axis.\n",
    "\n",
    "<font size=3>The header dictionary contains all information stored to the header section of the EDF file. Details on the exact meaning of each of these fields can be found here: https://www.edfplus.info/specs/edf.html. To ease access to the header data, the header is a dict instance that has been extended to include '.' dot notation attribute access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "154a9057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EEG EEG_1_SA-B', 'EEG EEG_2_SA-B', 'EEG EEG_3_SA-B', 'EEG EEG_4_SA-B', 'EDF Annotations']\n"
     ]
    }
   ],
   "source": [
    "# Fetch the names of the channels using '.' dot notation\n",
    "print(reader.header.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6481545",
   "metadata": {},
   "source": [
    "<font size=3>With the open reader instance, we can call the read method to read EDF data. To understand the parameters of this method lets ask for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d55dd090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method read in module openseize.file_io.edf:\n",
      "\n",
      "read(start: int, stop: Optional[int] = None, padvalue: float = nan) -> numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]] method of openseize.file_io.edf.Reader instance\n",
      "    Reads samples from this EDF from this Reader's channels.\n",
      "    \n",
      "    Args:\n",
      "        start:\n",
      "            The start sample index to read.\n",
      "        stop:\n",
      "            The stop sample index to read (exclusive). If None, samples\n",
      "            will be read until the end of file.\n",
      "        padvalue:\n",
      "            Value to pad to channels that run out of samples to return.\n",
      "            Only applicable if sample rates of channels differ.\n",
      "    \n",
      "    Returns:\n",
      "        A float64 array of shape len(chs) x (stop-start) samples.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(reader.read)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db79117b",
   "metadata": {},
   "source": [
    "<font size=3>The Readers read method reads from a <font color=firebrick>start sample</font> to a <font color=firebrick>stop sample</font> within the file. If the stop sample is not given the reader will read to the end of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64bd10be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-19.87908032,   7.95793213,  19.88808032,  18.89390131,\n",
       "         18.89390131],\n",
       "       [-86.4890744 ,  51.70180884,  63.63195703,  88.48643243,\n",
       "         63.63195703],\n",
       "       [-85.49489539,  44.74255573,  29.82987048,  79.53882129,\n",
       "         52.69598785],\n",
       "       [ 62.63777802,  95.44568555,  77.55046326,  36.7891236 ,\n",
       "        109.36419177]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read samples 0 to 5 for all 4 channels\n",
    "reader.read(0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af8fd0d",
   "metadata": {},
   "source": [
    "<font size=3>In addition to reading specific samples, the read method supports reading only a selection of <font color=firebrick>channels</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a31e9cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-19.87908032   7.95793213  19.88808032  18.89390131  18.89390131]\n",
      " [-85.49489539  44.74255573  29.82987048  79.53882129  52.69598785]]\n",
      "Next read will read channels =  [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "#set the reader to read only channels 0 and 2\n",
    "reader.channels = [0, 2]\n",
    "\n",
    "# read samples 0 to 5 for channels 0 and 2\n",
    "print(reader.read(0, 5))\n",
    "\n",
    "#set the reader to read all channels again\n",
    "reader.channels = reader.header.channels\n",
    "print('Next read will read channels = ', reader.channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76323ba",
   "metadata": {},
   "source": [
    "<font size=3>The EDF file specification allows for signals that may be sampled at different sample rates to be stored to the same file. In this case, a signal will have fewer samples than other signals in the file. In order to return non-ragged numpy arrays, the Reader will append the value of the <font color=firebrick>padvalue</font> parameter to shorter signals so that all signals have the same length. This padvalue defaults to np.NaN but may take on any value useful for your analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7465f8",
   "metadata": {},
   "source": [
    "### File Resources and Context Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81288a83",
   "metadata": {},
   "source": [
    "<font size=3>We have seen how to create a Reader instance and use it's read method to extract data from an EDF file. However, the file is still open and using resources that you need to recover. To do this you can call the Reader instance's <font color='firebrick'><b>close</b></font> method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5efb255",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1024462",
   "metadata": {},
   "source": [
    "<font size=3>To address this potential resource leak, openseize supports opening files using context managment. What does this mean?\n",
    "In python you open a text file using a piece of code that looks like this\n",
    "\n",
    "```py\n",
    "    with open('somefile.text', 'r') as infile:\n",
    "        process file\n",
    "```\n",
    "    \n",
    "<font size=3>When opened this way the file is automatically closed at the end of the \"with\" context. EDF Readers support opening EDF files in a context managed protocol too. <b>Here's how to open the file using the context manager protocol.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adb3a245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.98790803e+01  7.95793213e+00  1.98880803e+01 ...  4.50000000e-03\n",
      "   4.50000000e-03  4.50000000e-03]\n",
      " [-8.64890744e+01  5.17018088e+01  6.36319570e+01 ...  4.50000000e-03\n",
      "   4.50000000e-03  4.50000000e-03]\n",
      " [-8.54948954e+01  4.47425557e+01  2.98298705e+01 ...  4.50000000e-03\n",
      "   4.50000000e-03  4.50000000e-03]\n",
      " [ 6.26377780e+01  9.54456855e+01  7.75504633e+01 ...  4.50000000e-03\n",
      "   4.50000000e-03  4.50000000e-03]]\n",
      "\n",
      "ValueError: seek of closed file\n"
     ]
    }
   ],
   "source": [
    "# Open Reader as Context Manager and read data from within context\n",
    "with edf.Reader(filepath) as cmreader:\n",
    "    data = cmreader.read(0,)\n",
    "    print(data[:5])\n",
    "\n",
    "# Attempt to read from Reader after context has closed\n",
    "try:\n",
    "    cmreader.read(0,)\n",
    "except ValueError as err:\n",
    "    print(\"\\nValueError:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5629c8",
   "metadata": {},
   "source": [
    "<font size=3>This method of opening files inside a specific context and performing operations on the data is the preferred way to work with files in Openseize since the resources are automatically recovered at the end of the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e64e891",
   "metadata": {},
   "source": [
    "## Writing EDF Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7975412",
   "metadata": {},
   "source": [
    "<font size=3>In addition to an EDF file Reader, Openseize provides an <font color=firebrick>EDF file writer</font>. One of the use cases for this Writer is to split an EDF with channels corresponding to multiple subjects into multiple EDFs containing channels for only one subject. For example, if your EDF contains 3 subjects with 4 channels, their is a total of 12 signals in the EDF. The Writer can then be used to write 3 files each containing 4 channels. Lets examine how to use this Writer. <b>We'll again start by asking for help.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a00e2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Writer in module openseize.file_io.edf:\n",
      "\n",
      "class Writer(openseize.file_io.bases.Writer)\n",
      " |  Writer(path: Union[str, pathlib.Path]) -> None\n",
      " |  \n",
      " |  A writer of European Data Format (EDF/EDF+) files.\n",
      " |  \n",
      " |  This Writer is a context manager for writing EEG data and metadata to an\n",
      " |  EDF binary file. Unlike Readers it must be opened under the context\n",
      " |  management protocol. Importantly, this writer does not currently support\n",
      " |  writing annotations to an EDF file.\n",
      " |  \n",
      " |  Attributes:\n",
      " |      path (Path):\n",
      " |          A python path instance to target file to write data to.\n",
      " |  \n",
      " |  Examples:\n",
      " |      >>> from openseize.demos import paths\n",
      " |      >>> filepath = paths.locate('recording_001.edf')\n",
      " |      >>> # Create a reader that will read only channels [0, 1]\n",
      " |      >>> # and write out these channels to a new file\n",
      " |      >>> writepath = paths.data_dir.joinpath('subset_001.edf')\n",
      " |      >>> with Reader(filepath) as reader:\n",
      " |      >>>     with Writer(writepath) as writer:\n",
      " |      >>>         writer.write(reader.header, reader, channels=[0, 1])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Writer\n",
      " |      openseize.file_io.bases.Writer\n",
      " |      abc.ABC\n",
      " |      openseize.core.mixins.ViewInstance\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, path: Union[str, pathlib.Path]) -> None\n",
      " |      Initialize this Writer. See base class for futher details.\n",
      " |  \n",
      " |  write(self, header: openseize.file_io.edf.Header, data: Union[numpy.ndarray, openseize.file_io.edf.Reader], channels: Sequence[int], verbose: bool = True) -> None\n",
      " |      Write header metadata and data for channel in channels to this\n",
      " |      Writer's file instance.\n",
      " |      \n",
      " |      Args:\n",
      " |          header:\n",
      " |              A mapping of EDF compliant fields and values.\n",
      " |          data:\n",
      " |              An array with shape (channels, samples) or Reader instance.\n",
      " |          channels:\n",
      " |              Channel indices to write to this Writer's open file.\n",
      " |          verbose:\n",
      " |              An option to print progress of write.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueErrror: An error occurs if samples to be written is not\n",
      " |                       divisible by the number of records in the Header\n",
      " |                       instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from openseize.file_io.bases.Writer:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Return instance as target variable of this context.\n",
      " |  \n",
      " |  __exit__(self, exc_type, exc_value, traceback)\n",
      " |      Close this instances file object & propagate any error by\n",
      " |      returning None.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from openseize.file_io.bases.Writer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from openseize.core.mixins.ViewInstance:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Returns the __init__'s signature as the echo representation.\n",
      " |      \n",
      " |      Returns: str\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Returns this instances print representation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(edf.Writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94abee70",
   "metadata": {},
   "source": [
    "<font size=3>To construct a Writer instance you need to provide a file path where the writer will write the new EDF file to. The <font color=firebrick><b>write</b></font> method is what you will need to call in order to write data to the file path. <b>Lets examine this method by asking for the method's documentation.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f1b1a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function write in module openseize.file_io.edf:\n",
      "\n",
      "write(self, header: openseize.file_io.edf.Header, data: Union[numpy.ndarray, openseize.file_io.edf.Reader], channels: Sequence[int], verbose: bool = True) -> None\n",
      "    Write header metadata and data for channel in channels to this\n",
      "    Writer's file instance.\n",
      "    \n",
      "    Args:\n",
      "        header:\n",
      "            A mapping of EDF compliant fields and values.\n",
      "        data:\n",
      "            An array with shape (channels, samples) or Reader instance.\n",
      "        channels:\n",
      "            Channel indices to write to this Writer's open file.\n",
      "        verbose:\n",
      "            An option to print progress of write.\n",
      "    \n",
      "    Raises:\n",
      "        ValueErrror: An error occurs if samples to be written is not\n",
      "                     divisible by the number of records in the Header\n",
      "                     instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(edf.Writer.write)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f495a7b0",
   "metadata": {},
   "source": [
    "<font size=3>To write an EDF compliant file, the write method will need an <font color=firebrick>EDF Header instance</font> with all required fields and values expected of the EDF file type. An enumeration of the required fields and values can be found by examining the header printed above or be reading the EDF file specification here: https://www.edfplus.info/specs/edf.html\n",
    "    \n",
    "<font size=3>In addition to an EDF compliant Header instance, the write method needs <font color=firebrick>data</font>. This data may be an in-memory or a reader instance from which data will be fetched.\n",
    "    \n",
    "<font size=3>Lastly, the write method can take a <font color=firebrick>list of channel indices</font>. These channel indices will be used to filter both the Header instance and the data. For example, if you provide a Header containing metadata for 4 signals and an array containing 4 signals, you can request to write out a subset of the signals, say channel indices [0, 2]. This allows for the splitting of a multichannel EDF into multiple EDFs. Importantly, both the new data written and the new Header will contain only data and metadata for the 2 channels written. <b>Lets demonstrate these ideas with an example.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8520078f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writer Object\n",
      "---Attributes & Properties---\n",
      "{'path': PosixPath('/home/matt/python/nri/openseize/src/openseize/demos/data/subset_001.edf'),\n",
      " 'mode': 'wb'}\n",
      "\n",
      "Type help(Writer) for full documentation\n"
     ]
    }
   ],
   "source": [
    "# Create path for new EDF file to save to\n",
    "save_path = demos.paths.data_dir.joinpath('subset_001.edf')\n",
    "\n",
    "# Create an EDF writer pointing to this path\n",
    "writer = edf.Writer(save_path)\n",
    "print(writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387e4647",
   "metadata": {},
   "source": [
    "<font size=3>The writer knows where it will write data to and the write method can now be called to perform the writing. We will write channels 0 and 2 from the 'recording_001.edf' we used earlier. Since this file has a header, we will reuse that header. The write method will select metadata from the header corresponding to channels 0 and 2. The method will also only write data records corresponding to channels 0 and 2. <b><i>Remember to open the reader as a context manager so the file resources are automatically recovered.</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe753466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data: 100.0% complete\r"
     ]
    }
   ],
   "source": [
    "#locate the path to the recording\n",
    "fp = demos.paths.locate('recording_001.edf')\n",
    "\n",
    "#open the reader as context manager\n",
    "with edf.Reader(fp) as reader:\n",
    "    \n",
    "    #open the writer as a context manager\n",
    "    with edf.Writer(save_path) as writer:\n",
    "        \n",
    "        #write channels 0 and 2 from the header and reader's data\n",
    "        writer.write(reader.header, reader, channels=[0,2])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac448af",
   "metadata": {},
   "source": [
    "<font size=3>Notice here that we called both the Reader and Writer as context managers. Just like reader instances, writer instances maintain an open file to write to that is using your machines resources. By opening both the reader and writer as context managers, these file resources will be closed when the reading and writing is finished.</font>\n",
    "\n",
    "<font size=3><b>Now let's reopen the 'subset_001.edf' file we just wrote and make sure the header and data looks correct.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5232c27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---EDF SUBSET HEADER---\n",
      "{'version': '0',\n",
      " 'patient': 'PIN-42 M 11-MAR-1952 Animal',\n",
      " 'recording': 'Startdate 15-AUG-2020 X X X',\n",
      " 'start_date': '15.08.20',\n",
      " 'start_time': '09.59.15',\n",
      " 'header_bytes': 768,\n",
      " 'reserved_0': 'EDF+C',\n",
      " 'num_records': 3775,\n",
      " 'record_duration': 1.0,\n",
      " 'num_signals': 2,\n",
      " 'names': ['EEG EEG_1_SA-B', 'EEG EEG_3_SA-B'],\n",
      " 'transducers': ['8401 HS:15279', '8401 HS:15279'],\n",
      " 'physical_dim': ['uV', 'uV'],\n",
      " 'physical_min': [-8144.31, -8144.31],\n",
      " 'physical_max': [8144.319, 8144.319],\n",
      " 'digital_min': [-8192.0, -8192.0],\n",
      " 'digital_max': [8192.0, 8192.0],\n",
      " 'prefiltering': ['none', 'none'],\n",
      " 'samples_per_record': [5000, 5000],\n",
      " 'reserved_1': ['', '']}\n",
      "\n",
      "{'Accessible Properties': ['annotated', 'annotation', 'channels', 'offsets',\n",
      "                           'record_map', 'samples', 'slopes']}\n",
      "---EDF SUBSET DATA---\n",
      "[[-19.87908032   7.95793213  19.88808032  18.89390131  18.89390131]\n",
      " [-85.49489539  44.74255573  29.82987048  79.53882129  52.69598785]]\n"
     ]
    }
   ],
   "source": [
    "with edf.Reader(save_path) as reader:\n",
    "    \n",
    "    # lets print the readers Header-- it should only have metadata for channels 0 and 2\n",
    "    print('---EDF SUBSET HEADER---')\n",
    "    print(reader.header)\n",
    "    \n",
    "    #lets print the first 5 samples and check these against the full data\n",
    "    print('---EDF SUBSET DATA---')\n",
    "    print(reader.read(0,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6852a8",
   "metadata": {},
   "source": [
    "<font size=3>Both the header and the data appear to contain only the metadata and data for channels 0 and 2. Now lets check that is the case by examining all the data against the original 'recording_001.edf' demo file.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03d7de31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do the arrays match? ->  True\n"
     ]
    }
   ],
   "source": [
    "#fp is the still the filepath to recording_001.edf\n",
    "with edf.Reader(fp) as reader:\n",
    "    \n",
    "    #read all 4 channels from the file\n",
    "    all_data = reader.read(0)\n",
    "\n",
    "#save_path is where the subset_001.edf resides\n",
    "with edf.Reader(save_path) as reader:\n",
    "    \n",
    "    #read the 2 channels from the subset file\n",
    "    two_ch_data = reader.read(0)\n",
    "    \n",
    "print(\"Do the arrays match? -> \", np.allclose(all_data[[0,2], :], two_ch_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c4b760",
   "metadata": {},
   "source": [
    "## EDF Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87cace1",
   "metadata": {},
   "source": [
    "<font size=3>In addition to EDF file readers, Openseize provides <font color=firebrick>annotation file readers</font>. Typically, annotation files are comma-separated or tab-separated value text files that contain time-stamps and labels of important events that occurred during an EEG recording session. Here we will show how to open a Pinnacle format annotation TSV text file. <b>Let's start by looking at the documentation for this annotation reader.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19991e32",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Pinnacle in module openseize.file_io.annotations:\n",
      "\n",
      "class Pinnacle(openseize.file_io.bases.Annotations)\n",
      " |  Pinnacle(path: Union[str, pathlib.Path], **kwargs) -> None\n",
      " |  \n",
      " |  A reader of Pinnacle Technologies© annotation csv files.\n",
      " |  \n",
      " |  This annotation reader's 'read' method reads annotations into a list of\n",
      " |  Annotation dataclass instances. Each Annotation dataclass has the\n",
      " |  following attributes:\n",
      " |  \n",
      " |  - label: A string label given to an annotation.\n",
      " |  - time: Time, relative to recording start, in secs of an annotation.\n",
      " |  - duration: The duration in seconds of an annotation.\n",
      " |  - channel: The channel(s) an annotation was detected on.\n",
      " |  \n",
      " |  Attributes:\n",
      " |      path:\n",
      " |          Python path instance to Pinnacle© file.\n",
      " |      kwargs:\n",
      " |          Any valid kwarg for csv.DictReader initializer.\n",
      " |  \n",
      " |  Examples:\n",
      " |      >>> # read the annotations from the demo annotation file\n",
      " |      >>> from openseize.demos import paths\n",
      " |      >>> filepath = paths.locate('annotations_001.txt')\n",
      " |      >>> from openseize.io.annotations import Pinnacle\n",
      " |      >>> # read the 'rest' and 'exploring' annotations\n",
      " |      >>> with Pinnacle(filepath, start=6) as pinnacle:\n",
      " |      >>>     annotations = pinnacle.read(labels=['rest', 'exploring'])\n",
      " |      >>> # get the first annotation and print it\n",
      " |      >>> print(annotations[0])\n",
      " |      >>> # print the first annotations duration\n",
      " |      >>> print(annotations[0].duration)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Pinnacle\n",
      " |      openseize.file_io.bases.Annotations\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  channel(self, row: Dict[str, str]) -> Union[int, str]\n",
      " |      Extracts the annotation channel for a row in this file.\n",
      " |  \n",
      " |  duration(self, row: Dict[str, str]) -> float\n",
      " |      Measures the duration of an annotation for a row in this file.\n",
      " |  \n",
      " |  label(self, row: Dict[str, str]) -> str\n",
      " |      Extracts the annotation label for a row in this file.\n",
      " |  \n",
      " |  open(self, path: Union[str, pathlib.Path], start: int = 0, delimiter: str = '\\t', **kwargs) -> Tuple[IO[str], Iterable[dict]]\n",
      " |      Opens a Pinnacle formatted CSV annotation file.\n",
      " |      \n",
      " |      Called by 'Annotations.__init__' to initialize this Pinnacle\n",
      " |      context manager.\n",
      " |      \n",
      " |      Args:\n",
      " |          path:\n",
      " |              A annotation file path location.\n",
      " |          start:\n",
      " |              The row number of the column headers in the file.\n",
      " |          delimiter:\n",
      " |              The string character seperating columns in the file.\n",
      " |          **kwargs:\n",
      " |              Any valid keyword argument for CSV.DictReader builtin.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tuple (file_obj, DictReader) where file_obj is the open file\n",
      " |          instance and DictReader is the builtin csv DictReader.\n",
      " |  \n",
      " |  time(self, row: Dict[str, str]) -> float\n",
      " |      Extracts the annotation time of a row of this file.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from openseize.file_io.bases.Annotations:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Return this instance as target variable of this context.\n",
      " |  \n",
      " |  __exit__(self, exc_type, exc_value, traceback)\n",
      " |      Closes this instance's file obj. & propagate errors by returning\n",
      " |      None.\n",
      " |  \n",
      " |  __init__(self, path: Union[str, pathlib.Path], **kwargs) -> None\n",
      " |      Initialize this Annotations reader.\n",
      " |      \n",
      " |      Args:\n",
      " |          path:\n",
      " |              A path location to an annotation file.\n",
      " |          **kwargs:\n",
      " |              Any valid kwarg for a subclasses 'open' method.\n",
      " |  \n",
      " |  read(self, labels: Optional[Sequence[str]] = None) -> List[openseize.file_io.bases.Annotation]\n",
      " |      Reads annotations with labels to a list of Annotation instances.\n",
      " |      \n",
      " |      Args:\n",
      " |          labels:\n",
      " |              A sequence of annotation string labels for which Annotation\n",
      " |              instances will be returned. If None, return all.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of Annotation dataclass instances (see Annotation).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from openseize.file_io.bases.Annotations:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(annotations.Pinnacle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104f9372",
   "metadata": {},
   "source": [
    "<font size=3>To construct an annotation reader you will need to provide a <font color=firebrick>path to an annotation file</font>. This path is given to the open method (see above). Additionally, you may need to pass in a <font color=firebrick>start line of the file</font>. This describes what line the column data starts on. Lets fetch the demo file \"annotations_001.txt\" if it is not on your system already and display the file's contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9aad1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Experiment ID\tExperiment\n",
      "\n",
      "1 Animal ID\tAnimal\n",
      "\n",
      "2 Researcher\tTest\n",
      "\n",
      "3 Directory path\t\n",
      "\n",
      "4 \n",
      "\n",
      "5 \n",
      "\n",
      "6 Number\tStart Time\tEnd Time\tTime From Start\tChannel\tAnnotation\n",
      "\n",
      "7 0\t08/15/20 09:59:15.215\t08/15/20 09:59:15.215\t0.0000\tALL\tStarted Recording\n",
      "\n",
      "8 1\t08/15/20 10:00:00.000\t08/15/20 10:00:00.000\t44.7850\tALL\tQi_start\n",
      "\n",
      "9 2\t08/15/20 10:00:25.000\t08/15/20 10:00:30.000\t69.7850\tALL\tgrooming\n",
      "\n",
      "10 3\t08/15/20 10:00:45.000\t08/15/20 10:00:50.000\t89.7850\tALL\tgrooming\n",
      "\n",
      "11 4\t08/15/20 10:02:15.000\t08/15/20 10:02:20.000\t179.7850\tALL\tgrooming\n",
      "\n",
      "12 5\t08/15/20 10:04:36.000\t08/15/20 10:04:41.000\t320.7850\tALL\texploring\n",
      "\n",
      "13 6\t08/15/20 10:05:50.000\t08/15/20 10:05:55.000\t394.7850\tALL\texploring\n",
      "\n",
      "14 7\t08/15/20 10:08:50.000\t08/15/20 10:08:55.000\t574.7850\tALL\trest\n",
      "\n",
      "15 8\t08/15/20 10:10:14.000\t08/15/20 10:10:19.000\t658.7850\tALL\texploring\n",
      "\n",
      "16 9\t08/15/20 10:17:10.000\t08/15/20 10:17:15.000\t1074.7850\tALL\trest\n",
      "\n",
      "17 10\t08/15/20 10:35:49.000\t08/15/20 10:35:54.000\t2193.7850\tALL\trest\n",
      "\n",
      "18 11\t08/15/20 10:40:00.000\t08/15/20 10:40:00.000\t2444.7850\tALL\tQi_stop\n",
      "\n",
      "19 12\t08/15/20 11:02:09.879\t08/15/20 11:02:09.879\t3774.6640\tALL\tStopped Recording\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#determine the local path using the locate method and download if necessary\n",
    "annotations_path = demos.paths.locate('annotations_001.txt')\n",
    "\n",
    "#lets take a look at the file\n",
    "with open(annotations_path, 'r') as infile:\n",
    "    for idx, row in enumerate(infile):\n",
    "        print(idx, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c381107",
   "metadata": {},
   "source": [
    "<font size=3>With this path  we can now construct an Annotations reader instance. Just as with Readers and Writers <b><i>an instance can (and most of the time should) be constructed as a context manager.</i></b> Below we are going to construct the annotations reader starting from line 6 since that is the row containing the column headers of the file. Note this initialization argument is passed to the open method which can accept any argument that python's builtin CSV.DictReader can accept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecd73063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation(label='Started Recording', time=0.0, duration=0.0, channel='ALL')\n",
      "Annotation(label='Qi_start', time=44.785, duration=0.0, channel='ALL')\n",
      "Annotation(label='grooming', time=69.785, duration=5.0, channel='ALL')\n",
      "Annotation(label='grooming', time=89.785, duration=5.0, channel='ALL')\n",
      "Annotation(label='grooming', time=179.785, duration=5.0, channel='ALL')\n",
      "Annotation(label='exploring', time=320.785, duration=5.0, channel='ALL')\n",
      "Annotation(label='exploring', time=394.785, duration=5.0, channel='ALL')\n",
      "Annotation(label='rest', time=574.785, duration=5.0, channel='ALL')\n",
      "Annotation(label='exploring', time=658.785, duration=5.0, channel='ALL')\n",
      "Annotation(label='rest', time=1074.785, duration=5.0, channel='ALL')\n",
      "Annotation(label='rest', time=2193.785, duration=5.0, channel='ALL')\n",
      "Annotation(label='Qi_stop', time=2444.785, duration=0.0, channel='ALL')\n",
      "Annotation(label='Stopped Recording', time=3774.664, duration=0.0, channel='ALL')\n"
     ]
    }
   ],
   "source": [
    "#open the annotations and read all the annotations in the file using the 'read' method\n",
    "with annotations.Pinnacle(annotations_path, start=6) as reader:\n",
    "    \n",
    "    #call read to get the annotations as a sequence of Annotation instances (to be described in a moment)\n",
    "    annotes = reader.read()\n",
    "    \n",
    "#print the sequence of annotation instances\n",
    "for instance in annotes:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208f4d1d",
   "metadata": {},
   "source": [
    "<font size=3>You can see that we have fetched all of the annotations from the displayed file and stored each annotation to an <font color=firebrick>Annotation instance</font>. What is this instance? An Annotation object is a python dataclass. If you haven't seen this before, you can think of it as a simple container with '.' dot notation access to the container's contents. <b>Let's examime the third dataclass instance.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27597184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation(label='grooming', time=89.785, duration=5.0, channel='ALL')\n",
      "This annotation occurred at 89.785 s relative to the start time\n"
     ]
    }
   ],
   "source": [
    "#fetch the third annotation item and display it\n",
    "item = annotes[3]\n",
    "print(item)\n",
    "\n",
    "#access the items time from recording start\n",
    "print('This annotation occurred at {} s relative to the start time'.format(item.time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ed767d",
   "metadata": {},
   "source": [
    "<font size=3>The key pieces of information are given to us in a single Annotation instance:\n",
    " - <font color=firebrick><b>label</b></font> - a piece of text describing the annotation\n",
    " - <font color=firebrick><b>time</b></font> - the exact point in time (in seconds) from the beginning of the recording that the annotation takes place\n",
    " - <font color=firebrick><b>duration</b></font> - the length (in seconds) of the annotation from its start time \n",
    " - <font color=firebrick><b>channel</b></font> - a list of the channels in the EEG recording that the annotation is applied to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e5ba75",
   "metadata": {},
   "source": [
    "<font size=3>In the preceding example, we read all of the annotations from the Pinnacle formatted file but the Annotations 'read' method can accept a sequence of <font color=firebrick>labels</font> to selectively read only some of the annotations. <b>Let's show how this works on this demo annotation file.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9b90106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation(label='exploring', time=320.785, duration=5.0, channel='ALL')\n",
      "Annotation(label='exploring', time=394.785, duration=5.0, channel='ALL')\n",
      "Annotation(label='rest', time=574.785, duration=5.0, channel='ALL')\n",
      "Annotation(label='exploring', time=658.785, duration=5.0, channel='ALL')\n",
      "Annotation(label='rest', time=1074.785, duration=5.0, channel='ALL')\n",
      "Annotation(label='rest', time=2193.785, duration=5.0, channel='ALL')\n"
     ]
    }
   ],
   "source": [
    "#read only the annotations with labels matching either rest or exploring\n",
    "with annotations.Pinnacle(annotations_path, start=6) as reader:\n",
    "    subset_annotes = reader.read(labels=['rest', 'exploring'])\n",
    "    \n",
    "for annote in subset_annotes:\n",
    "    print(annote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feffa78",
   "metadata": {},
   "source": [
    "## Producing from EDF Files with Annotations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec43b80e",
   "metadata": {},
   "source": [
    "<font size=3>Two important components of an Annotation instance is the <font color=firebrick><b>time</b></font> and <font color=firebrick><b>duration</b></font> attributes. These attributes allow for selective filtering of EEG data returned from either a Reader or a producer. To do this, the annotation dataclass  instances are converted into a boolean mask that can pick out samples of data to keep or discard. <b>Here we will demonstrate how to construct a boolean mask from a list of annotation instances and use that mask to filter a producer's yielded numpy arrays.</b> Further details can be found in the producer demo. \n",
    "\n",
    "<font size=3>The annotations module provides a method for generating a mask automatically from a series of annotation objects, the <font color=firebrick><b>as_mask</b></font> method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43f4475f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function as_mask in module openseize.file_io.annotations:\n",
      "\n",
      "as_mask(annotations: Sequence[openseize.file_io.bases.Annotation], size: int, fs: float, include: bool = True) -> numpy.ndarray[typing.Any, numpy.dtype[numpy.bool_]]\n",
      "    Creates a boolean mask from a sequence of annotation dataclass\n",
      "    instances..\n",
      "    \n",
      "    Producers of EEG data may recieve an optional boolean array mask.  This\n",
      "    function creates a boolean mask from a sequence of annotations and is\n",
      "    therefore useful for filtering EEG data by annotation label during\n",
      "    processing.\n",
      "    \n",
      "    Args:\n",
      "        annotations:\n",
      "            A sequence of annotation dataclass instances to convert to a \n",
      "            mask.\n",
      "        size:\n",
      "            The length of the boolean array to return.\n",
      "        fs:\n",
      "            The sampling rate in Hz of the digital system.\n",
      "        include:\n",
      "            Boolean determining if annotations should be set to True or\n",
      "            False in the returned array. True means all values\n",
      "            are False in the returned array except for samples where the\n",
      "            annotations are located.\n",
      "    \n",
      "    Returns:\n",
      "        A 1-D boolean array of length size.\n",
      "    \n",
      "    Examples:\n",
      "        >>> # read the annotations from the demo annotation file\n",
      "        >>> from openseize.demos import paths\n",
      "        >>> filepath = paths.locate('annotations_001.txt')\n",
      "        >>> from openseize.io.annotations import Pinnacle\n",
      "        >>> # read the 'rest' anotations\n",
      "        >>> with Pinnacle(filepath, start=6) as pinnacle:\n",
      "        >>>     annotations = pinnacle.read(labels=['rest'])\n",
      "        >>> # create a mask measuring 3700 secs at 5000 Hz\n",
      "        >>> mask = as_mask(annotations, size=3700*5000, fs=5000)\n",
      "        >>> # measure the total time in secs of 'rest' annotation\n",
      "        >>> print(np.count_nonzero(mask) / 5000)\n",
      "        15.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(annotations.as_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd97d2",
   "metadata": {},
   "source": [
    "<font size=3>To construct a mask, <font color=firebrick><b>as_mask</b></font> needs a sequence of <font color=firebrick>annotation dataclass instances</font>, the <font color=firebrick>size</font> of the mask along the sample axis, the <font color=firebrick>sampling rate</font> to convert the annotation times to samples, and a <font color=firebrick>boolean \"include\" parameter</font> which determines if the annotations should be kept (True) or discarded (False) from the EEG data.\n",
    "<font size=3>Here, as an example, <b>we create such a mask.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63b0d898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the annotations from the demo file\n",
    "with annotations.Pinnacle(annotations_path, start=6) as reader:\n",
    "    subset_annotes = reader.read(labels=['rest', 'exploring'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df93db70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the mask; fp is the still the filepath to recording_001.edf; size and fs can be fetched from reader\n",
    "with edf.Reader(fp) as reader:    \n",
    "    size = reader.shape[-1]\n",
    "    fs = reader.header.samples_per_record[0]\n",
    "    \n",
    "mask = annotations.as_mask(subset_annotes, size, fs, include=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91209e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False False]\n",
      "[False False False False False  True  True  True  True  True]\n",
      "Expected number of samples to keep is 150000 \n",
      "Actual number kept is 150000\n"
     ]
    }
   ],
   "source": [
    "#print the first 10 values of the mask\n",
    "print(mask[:10])\n",
    "\n",
    "#The first True values should occur at 320.785 secs * 5000 Hz since fs=5000 and the first annotation \n",
    "#(see above occurs at 320.785). Lets confirm this by print 10 samples around this sample\n",
    "start = int(320.785 * 5000)\n",
    "print(mask[start-5: start+5])\n",
    "\n",
    "#lastly lets print out the total number of samples we will keep\n",
    "expected = len(subset_annotes) * 5 * 5000 # each annote is 5 secs @ 5 kHz\n",
    "actual = np.count_nonzero(mask)\n",
    "print('Expected number of samples to keep is {} \\nActual number kept is {}'.format(expected, actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70d07c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producer Shape (w/ mask applied): (4, 150000)\n"
     ]
    }
   ],
   "source": [
    "# Build a producer with this mask and show that it has the expected shape\n",
    "with edf.Reader(filepath) as reader:\n",
    "    masked_producer = producer(reader.read(start=6), chunksize=500, axis=-1, mask=mask)   \n",
    "    print(\"Producer Shape (w/ mask applied):\",masked_producer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0e9eef",
   "metadata": {},
   "source": [
    "<font size=3>As we can see, the producer maintains four channels, with 150000 records each, exactly as we anticipated from our mask."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
